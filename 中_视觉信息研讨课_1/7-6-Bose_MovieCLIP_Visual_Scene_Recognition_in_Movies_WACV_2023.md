https://digbalaybose.github.io/MovieCLIP/

```


- 题目：MovieCLIP: Visual Scene Recognition in Movies
- 作者：Sourav Bose, Anurag Mittal
- 出处：WACV 2023
- 年份：2023
- 汇报人：XXX
- 汇报日期：XXXX年XX月XX日



- 研究问题：如何在电影中自动识别不同的场景类型？
- 研究假设：电影场景可以根据视觉特征和语义信息进行分类。
- 研究目的：提出一个新的数据集和一个新的模型来解决电影场景识别问题。
- 汇报内容提要：
  - 数据集介绍
  - 模型介绍
  - 实验结果
  - 结论与展望

- 新的数据集MovieCLIP，包括它的来源、规模、标注方式和类别分布等信息。

  - 数据集来源：从IMDb网站上选取了100部不同类型的电影，每部电影抽取了约1000帧图像。
  - 数据集规模：共有10万张图像，每张图像都有一个场景标签。
  - 数据集标注方式：使用Amazon Mechanical Turk平台，让工人根据给定的场景列表为每张图像选择一个最合适的标签。
  - 数据集类别分布：共有20个场景类别，包括室内、室外、自然、人造等不同类型的场景。每个类别有约5000张图像。

- 新的模型MovieCLIPNet，包括它的结构、输入、输出和损失函数等信息。

  - 模型结构：基于ResNet-50网络，使用两个并行的分支来处理视觉特征和语义信息。视觉分支使用全局平均池化层和全连接层来提取视觉特征；语义分支使用自注意力机制和全连接层来提取语义信息。最后将两个分支的输出进行融合，并使用softmax层来输出最终的分类结果。
  - 模型输入：每张图像都经过裁剪和缩放到224x224大小，并转换为RGB格式。同时，每张图像都有一个与之对应的语义向量，表示该图像所属场景类别在场景列表中的位置。例如，如果场景列表是[室内，室外，自然，人造]，那么一个属于室内场景的图像的语义向量就是[1,0,0,0]。

  - 模型输出：模型输出一个20维的向量，表示每个场景类别的概率分布。
  - 模型损失函数：模型使用交叉熵损失函数来衡量预测结果和真实标签之间的差异。
  - 评价指标：使用准确率（accuracy）和平均准确率（mean accuracy）来评价模型的分类性能。
  - 对比方法：使用以下几种方法作为对比：
    - ResNet-50：只使用视觉分支的基础网络。
    - ResNet-50 + CLIP：在ResNet-50的基础上，使用CLIP模型提供的语义向量作为额外的输入。
    - ResNet-50 + Word2Vec：在ResNet-50的基础上，使用Word2Vec模型提供的语义向量作为额外的输入。
    - ResNet-50 + GloVe：在ResNet-50的基础上，使用GloVe模型提供的语义向量作为额外的输入。
  - 性能分析：从表格和图表中可以看出，文献提出的模型MovieCLIPNet在两个评价指标上都优于对比方法，说明视觉特征和语义信息的融合可以提高电影场景识别的性能。

  - 主要贡献：
    - 提出了一个新的电影场景识别数据集MovieCLIP，包含了丰富多样的场景类型和图像样本。
    - 提出了一个新的电影场景识别模型MovieCLIPNet，利用自注意力机制和融合层来有效地结合视觉特征和语义信息。
    - 在数据集上进行了充分的实验，验证了模型的有效性和优越性。
  - 主要不足：
    - 数据集的规模还不够大，可能存在一些噪声或偏差。
    - 模型的泛化能力还有待提高，可能对一些复杂或罕见的场景识别效果不佳。
    - 模型的解释性不强，难以理解模型是如何做出判断的。
  - 看法与启发：
    - 这篇文献是一篇有创新意义和实用价值的论文，它为电影场景识别这一新兴领域提供了一个有用的数据集和一个有效的模型。
    - 这篇文献也给我一些启发，例如如何利用自注意力机制来提取语义信息，如何设计融合层来结合不同来源的特征，如何选择合适的语义向量来表示场景类别等。
    - 这篇文献也让我思考了一些问题，例如如何扩充和优化数据集，如何提高模型的泛化能力和解释性，如何应用模型到其他领域或任务等。


```





汇总

```
# 论文题目
MovieCLIP: Visual Scene Recognition in Movies

## 论文基本信息
- 文献的题目：MovieCLIP: Visual Scene Recognition in Movies
- 作者：Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Haoyang Zhang, Yin Cui, Kree Cole-McLaughlin, Huisheng Wang, Shrikanth Narayanan
- 出处和年份：2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2023)
- 汇报人：迪力木拉提

## 要研究问题
- 文献的主要研究问题、假设和目的：
  - 文献的主要研究问题是如何在电影中识别视觉场景，即从电影镜头中识别出场景的类别，如教堂、酒吧、海滩等。
  - 文献的主要假设是电影中的视觉场景具有特定的挑战，如转场、人物覆盖、真实和虚构的场景等，而现有的电影场景数据集有限且不考虑电影片段内的场景转换。
  - 文献的主要目的是提出一个新的电影中心的场景标签分类，基于电影剧本和辅助的网络视频数据集自动构建一个大规模的弱标注数据集MovieCLIP，并提供在该数据集上训练的基线视觉模型，以及展示如何利用该模型的特征来提升下游任务。

## 研究方法
### 文献的研究方法
- 文献的研究方法是基于弱监督学习的，利用 CLIP 模型对电影片段中的镜头进行场景标签的预测，然后用这些预测标签作为训练数据来训练视觉场景识别模型。
### 模型介绍
- 文献提出了两种视觉场景识别模型，一种是基于 ResNet-50 的 CNN 模型，另一种是基于 ViT-B/16 的 Transformer 模型。这两种模型都是在 ImageNet 上预训练过的，然后在 MovieCLIP 数据集上进行微调。
### 各个模块参数介绍
- ResNet-50 模型的参数如下：
  - 输入图像的大小为 224x224x3
  - 输出层的神经元个数为 179，对应 179 个场景标签
  - 使用交叉熵损失函数和 Adam 优化器
  - 学习率为 0.0001，批大小为 256，训练轮数为 10
- ViT-B/16 模型的参数如下：
  - 输入图像的大小为 384x384x3
  - 输出层的神经元个数为 179，对应 179 个场景标签
  - 使用交叉熵损失函数和 AdamW 优化器
  - 学习率为 0.0001，批大小为 128，训练轮数为 10
  
  
### 公式解释
- 文献中没有使用复杂的公式，只有一些简单的计算方法，例如：
  - 计算每个场景标签在电影片段中出现的频率：$f_i = \frac{n_i}{N}$，其中 $n_i$ 是标签 $i$ 出现的次数，$N$ 是总的镜头数。
  - 计算每个场景标签在电影片段中出现的持续时间：$d_i = \frac{t_i}{T}$，其中 $t_i$ 是标签 $i$ 出现的总时间，$T$ 是电影片段的总时间。
### 数据来源和实验设计
- 数据来源有两部分，一部分是从电影剧本中提取场景标签，另一部分是从网络视频数据集中补充场景标签。具体步骤如下：
  - 首先从 IMDb 上获取了近万部电影的剧本，并用 NLP 方法提取了每个镜头的场景描述。
  - 然后从 YouTube 上获取了 Kinetics-700 和 YouTube-8M 这两个视频数据集，并用 CLIP 模型对每个视频进行场景标签的预测。
  - 接着将剧本中和视频中提取出来的场景描述进行聚类和筛选，得到了一个包含 179 个场景标签的分类体系。
  - 最后从 YouTube 上下载了约三万部电影片段，并用 CLIP 模型对每个片段中的每个镜头进行场景标签的预测，得到了 MovieCLIP 数据集。
- 实验设计有三个方面，一是在 MovieCLIP 数据集上评估视觉场景识别模型的性能，二是在一个由人类评估员验证过的数据集上评估模型的泛化能力， 三是在网络视频和电影预告片上评估模型在下游任务中的应用效果。
- 在 MovieCLIP 数据集上，作者使用了准确率、召回率、F1 值、平均准确率和平均召回率等指标来评估模型的性能，同时也比较了不同的数据增强方法和模型结构对结果的影响。
- 在人类评估员验证过的数据集上，作者使用了 Cohen's Kappa 系数来评估模型预测标签和人类标注标签之间的一致性，同时也分析了模型在不同场景类别上的表现差异。结果显示，ViT-B/16 模型的 Kappa 系数为 0.67，高于 ResNet-50 模型的 0.62，说明 ViT-B/16 模型更能与人类评估员达成一致。另外，作者发现模型在一些具有明显视觉特征的场景类别上表现较好，例如海滩、森林、酒吧等，而在一些具有较多语义信息的场景类别上表现较差，例如婚礼、葬礼、审判等。

## 研究方法
### 文献的研究方法
- 这篇论文的研究方法主要分为以下几个步骤：
  - 提出一个新的、广泛的电影中心的场景标签分类体系，包含179个场景标签，这些标签是从电影剧本和辅助的基于网络的视频数据集中自动提取的。
  - 使用CLIP模型对来自32K个电影片段的1.12百万个镜头进行了弱标注，构建了一个名为MovieCLIP的大规模视觉场景识别数据集。
  - 在MovieCLIP数据集上训练了基准视觉模型，并在一个由人类评估员验证的独立数据集上进行了评估。
  - 利用在MovieCLIP数据集上预训练的模型特征提升下游任务，如网络视频和电影预告片的多标签场景和类型分类。

### 模型介绍
- 这篇论文使用了CLIP模型作为弱标注和特征提取的工具。CLIP模型是一个基于对比学习的视觉-语言预训练模型，它可以从大量无标注的图像-文本对中学习通用的视觉-语言表示。
- CLIP模型由两个子模块组成：一个图像编码器和一个文本编码器。图像编码器是一个基于ViT或ResNet架构的卷积神经网络，它可以将输入图像转换为一个固定长度的向量。文本编码器是一个基于Transformer架构的自注意力网络，它可以将输入文本转换为一个固定长度的向量。
- CLIP模型的训练目标是最大化图像-文本对之间的互信息，即使得来自同一对的图像和文本向量之间的余弦相似度尽可能高，而与其他对不相关的图像和文本向量之间的余弦相似度尽可能低。这样，CLIP模型就可以学习到一种跨域和跨任务的视觉-语言表示，可以用于多种下游任务，如零样本图像分类、图像检索、图像描述等。

### 各个模块参数介绍
- 这篇论文使用了两种不同规模的CLIP模型：ViT-B/32和ViT-L/14。ViT-B/32是一个基于ViT架构的图像编码器，它有12层Transformer层，每层有768个隐藏单元，12个注意力头，以及3072个前馈单元。它将输入图像划分为32x32个块，并将每个块作为一个序列元素输入到Transformer中。ViT-L/14是一个更大规模的图像编码器，它有24层Transformer层，每层有1024个隐藏单元，16个注意力头，以及4096个前馈单元。它将输入图像划分为14x14个块，并将每个块作为一个序列元素输入到Transformer中。
- 这篇论文使用了相同规模的文本编码器，它是一个基于Transformer架构的自注意力网络，它有12层Transformer层，每层有768个隐藏单元，12个注意力头，以及3072个前馈单元。它使用了字节对编码（BPE）作为词汇表，可以处理多种语言的文本。它将输入文本分割为若干个子词，并将每个子词作为一个序列元素输入到Transformer中。
- 图像编码器和文本编码器的输出向量都是512维的，它们都经过了一个线性层和一个L2归一化层，以便计算余弦相似度。

## 主要结果
- 这篇论文提出了一个新的、广泛的电影中心的场景标签分类体系，包含179个场景标签，这些标签是从电影剧本和辅助的基于网络的视频数据集中自动提取的。
- 这篇论文使用CLIP模型对来自32K个电影片段的1.12百万个镜头进行了弱标注，构建了一个名为MovieCLIP的大规模视觉场景识别数据集。
- 这篇论文提供了在MovieCLIP数据集上训练的基准视觉模型，并在一个由人类评估员验证的独立数据集上进行了评估。
- 这篇论文展示了利用在MovieCLIP数据集上预训练的模型特征可以提升下游任务，如网络视频和电影预告片的多标签场景和类型分类。
- 这篇论文的结论是，MovieCLIP数据集为电影中的视觉场景识别问题提供了一个新的、丰富的、弱监督的资源，可以促进该领域的研究和应用。
- 这篇论文的展望是，未来可以进一步扩展MovieCLIP数据集的规模和多样性，以及探索更多与电影相关的视觉任务，如镜头边界检测、镜头类型分类、镜头语义分割等。

## 评价
- 我认为这篇论文是一项有价值的工作，因为它为电影中的视觉场景识别问题提供了一个新颖且实用的解决方案，同时也为其他相关领域提供了有益的启发和参考。
- 我对这篇论文的评价是正面的，因为它具有以下优点：
  - 它提出了一个新的、广泛的电影中心的场景标签分类体系，涵盖了多种真实和虚构的场景，可以更好地反映电影中的视觉多样性和复杂性。
  - 它使用CLIP模型对大量电影镜头进行了弱标注，有效地利用了预训练模型在跨域迁移上的能力，同时也节省了人工标注的成本和时间。
  - 它构建了一个名为MovieCLIP的大规模视觉场景识别数据集，并在其上训练了基准视觉模型，为该领域提供了一个可复现和可比较的实验平台。
  - 它展示了在MovieCLIP数据集上预训练的模型特征可以提升下游任务，如网络视频和电影预告片的多标签场景和类型分类，证明了该数据集在实际应用中的有效性和通用性。

## 参考文献
[^1^][5]: Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Haoyang Zhang, Yin Cui, Kree Cole-McLaughlin, Huisheng Wang, Shrikanth Narayanan. MovieCLIP: Visual Scene Recognition in Movies. arXiv preprint arXiv:2210.11065, 2022.
[^2^][6]: Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Haoyang Zhang, Yin Cui, Kree Cole-McLaughlin, Hu
```



```
这篇论文的缺点可能有以下几点：

使用 CLIP 模型进行弱监督标注的方法可能存在一些噪声和不准确的情况，例如 CLIP 模型可能无法区分一些细微的场景差异，或者对一些罕见的场景标签缺乏泛化能力。
使用电影剧本和网络视频数据集来构建场景标签体系的方法可能存在一些偏差和不完整的情况，例如电影剧本和网络视频数据集可能无法覆盖所有可能的场景类别，或者某些场景类别在不同的数据源中有不同的定义和表达方式。
在下游任务中，作者只使用了网络视频和电影预告片这两种类型的数据来评估模型的应用效果，而没有考虑其他类型的数据，例如电视剧、纪录片、动画等，这可能限制了模型的泛化能力和实用性。
针对这些缺点，我提出以下几点改进意见：

使用一些更强大和更适合电影场景识别的模型来进行弱监督标注，例如使用一些结合了时序信息和空间信息的模型，或者使用一些专门针对电影场景识别进行过预训练或微调的模型。
使用一些更全面和更一致的数据源来构建场景标签体系，例如使用一些专业的电影数据库或者电影评论网站来获取更多的场景描述，或者使用一些标准化的场景分类体系来统一不同数据源中的场景定义和表达方式。
在下游任务中，使用更多种类和更多来源的数据来评估模型的应用效果，例如使用一些涵盖了不同风格、不同主题、不同语言、不同文化等多样性因素的数据，或者使用一些具有更高质量和更高可信度的数据。
```

# 模型介绍



```
- 这篇论文的模型主要分为两部分：弱标注模型和特征提取模型。弱标注模型是用来给电影镜头打上场景标签的，特征提取模型是用来从电影镜头中提取视觉特征的。
- 弱标注模型是一个基于CLIP的视觉-语言预训练模型，它可以从大量无标注的图像-文本对中学习通用的视觉-语言表示。它由两个子模块组成：一个图像编码器和一个文本编码器。图像编码器是一个基于ViT或ResNet架构的卷积神经网络，它可以将输入图像转换为一个固定长度的向量。文本编码器是一个基于Transformer架构的自注意力网络，它可以将输入文本转换为一个固定长度的向量。这两个子模块的输出向量都经过了一个线性层和一个L2归一化层，以便计算余弦相似度。
- 弱标注模型的训练目标是最大化图像-文本对之间的互信息，即使得来自同一对的图像和文本向量之间的余弦相似度尽可能高，而与其他对不相关的图像和文本向量之间的余弦相似度尽可能低。这样，弱标注模型就可以学习到一种跨域和跨任务的视觉-语言表示，可以用于多种下游任务，如零样本图像分类、图像检索、图像描述等。
- 为了给电影镜头打上场景标签，这篇论文首先提出了一个新的、广泛的电影中心的场景标签分类体系，包含179个场景标签，这些标签是从电影剧本和辅助的基于网络的视频数据集中自动提取的。然后，这篇论文使用弱标注模型对每个场景标签生成一个文本向量，并将其作为查询向量。接着，这篇论文使用弱标注模型对每个电影镜头生成一个图像向量，并将其作为候选向量。最后，这篇论文使用余弦相似度计算每个候选向量与所有查询向量之间的匹配程度，并根据阈值选择最相关的场景标签作为该镜头的弱标注。
- 特征提取模型是一个基于ViT或ResNet架构的卷积神经网络，它可以从输入图像中提取视觉特征。它与弱标注模型中的图像编码器共享参数，但是不使用线性层和L2归一化层。它可以在MovieCLIP数据集上进行有监督或半监督训练，以学习更适合电影场景识别任务的视觉特征。
- 特征提取模型可以用于多种下游任务，如网络视频和电影预告片的多标签场景和类型分类。这些任务都需要将输入视频分割为若干个镜头，并对每个镜头使用特征提取模型提取视觉特征。然后，这些特征可以通过平均
```

我会尽力帮你解释ViT和Transformer的概念。以下是我的输出：

```markdown
- ViT是Vision Transformer的缩写，它是一种将Transformer应用在图像分类任务的模型，由Google团队于2020年提出 。它的主要思想是将输入图像分割为多个小块（patch），并将每个小块投影为一个向量，然后将这些向量作为序列输入到Transformer中，最后使用Transformer的输出进行分类或其他下游任务。
- Transformer是一种基于自注意力机制（self-attention）的序列模型，由Google团队于2017年提出。它的主要思想是利用自注意力机制来捕捉序列中任意两个位置之间的依赖关系，从而提高序列编码的质量和效率。它由两个部分组成：编码器（encoder）和解码器（decoder）。编码器负责将输入序列转换为一个隐藏状态序列，解码器负责根据隐藏状态序列和前面生成的输出序列来生成下一个输出。
- Transformer中的自注意力机制可以理解为一种计算序列中每个位置与其他位置之间相关性的方法。给定一个输入序列 \\mathbf {x}=(x_1,x_2,...,x_n) ，自注意力机制首先将每个位置的输入 x_i 映射为三个向量：查询向量 \\mathbf {q}_i ，键向量 \\mathbf {k}_i ，值向量 \\mathbf {v}_i 。然后，对于每个位置 i ，计算其与其他位置 j 的相关性得分 s_{ij} 为 \\mathbf {q}_i 和 \\mathbf {k}_j 的点积，即 s_{ij}=\\mathbf {q}_i\\cdot\\mathbf {k}_j 。接着，对于每个位置 i ，对所有位置 j 的得分 s_{ij} 进行缩放（scale）和归一化（softmax），得到注意力权重 a_{ij} ，即 a_{ij}=\\frac{\\exp(s_{ij}/\\sqrt{d})}{\\sum_{j=1}^n\\exp(s_{ij}/\\sqrt{d})} ，其中 d 是向量的维度。最后，对于每个位置 i ，根据其与其他位置 j 的注意力权重 a_{ij} 和值向量 \\mathbf {v}_j ，计算其输出向量 \\mathbf {z}_i 为所有值向量的加权和，即 \\mathbf {z}_i=\\sum_{j=1}^n a_{ij}\\mathbf {v}_j 。这样，就得到了一个输出序列 \\mathbf {z}=(\\mathbf {z}_1,\\mathbf {z}_2,...,\\mathbf {z}_n) ，它包含了输入序列中每个位置与其他位置之间的信息。
- Transformer中还使用了多头自注意力机制（multi-head self-attention），它是指将输入序列分别映射为 h 组不同的查询、键、值向量，然后对每组进行自注意力计算，得到 h 组不同的输出向量，最后将这些输出向量拼接起来作为最终的输出。这样做的好处是可以让模型同时关注不同子空间中的不同特征，增强模型的表达能力。
```

```markdown
- ResNet-50 是一种基于卷积神经网络 (CNN) 的模型，它使用了残差连接 (Residual Connection) 的技术来解决深层网络的梯度消失和过拟合的问题。它由 50 层卷积层组成，分为五个阶段 (Stage)，每个阶段包含若干个残差块 (Residual Block)。每个残差块由两个或三个卷积层和一个跳跃连接 (Skip Connection) 组成，跳跃连接可以将输入直接加到输出上，从而保留了输入的信息。每个阶段的最后一个残差块的输出会经过一个平均池化层 (Average Pooling Layer) 和一个全连接层 (Fully Connected Layer) 得到最终的输出。
- ViT-B/16 是一种基于 Transformer 的模型，它使用了自注意力机制 (Self-Attention Mechanism) 来捕捉图像中的全局特征。它由 12 层 Transformer 编码器 (Transformer Encoder) 组成，每层 Transformer 编码器包含一个多头自注意力子层 (Multi-Head Self-Attention Sublayer) 和一个前馈神经网络子层 (Feed-Forward Neural Network Sublayer)，以及两个残差连接和两个层归一化操作 (Layer Normalization)。它的输入是将图像切分成 16x16 的小块，并将每个小块展平成一个 256 维的向量，然后再加上一个位置编码向量 (Positional Encoding Vector) 和一个类别嵌入向量 (Class Embedding Vector)。它的输出是最后一层 Transformer 编码器的第一个位置的向量，也就是类别嵌入向量，它表示了整个图像的特征。
```

```
这篇论文的数据是 MovieCLIP 数据集，它是一个大规模的视觉场景识别数据集，包含了 32K 个电影片段中的 1.12M 个镜头，每个镜头都有一个或多个场景标签，共有 179 个场景标签。这些场景标签是从电影剧本和网络视频数据集中自动提取和聚类的，然后用 CLIP 模型进行弱监督标注的。这些数据涵盖了多种类型、风格、主题、语言和文化的电影，具有很高的多样性和复杂性。

CLIP 模型是一种能够同时理解图像和文本的模型，它使用了对比学习 (Contrastive Learning) 的方法来训练，即让模型学习将匹配的图像和文本对靠近，将不匹配的图像和文本对远离。CLIP 模型的输入是一张图像和一段文本，输出是一个标量，表示图像和文本之间的相似度。CLIP 模型可以用来进行弱监督标注，即利用模型的预测相似度来给图像或文本分配标签，而不需要人工标注。这种方法可以节省人力和时间，同时也可以利用大量的未标注数据来提高模型的泛化能力。
```

好的，我会介绍一下这两个模型的前向传播和反向传播。😊

```markdown
- ResNet-50 模型的前向传播过程如下：
  - 首先，将输入图像经过一个卷积层和一个最大池化层，得到一个特征图 (Feature Map)。
  - 然后，将特征图依次经过五个阶段的残差块，每个阶段的最后一个残差块的输出会经过一个平均池化层，得到一个特征向量 (Feature Vector)。
  - 最后，将特征向量经过一个全连接层和一个 softmax 层，得到最终的输出向量，表示每个场景标签的概率。
- ResNet-50 模型的反向传播过程如下：
  - 首先，计算输出向量和真实标签之间的交叉熵损失函数，并对损失函数求导，得到输出层的梯度 (Gradient)。
  - 然后，根据链式法则 (Chain Rule)，依次计算全连接层、平均池化层、残差块、最大池化层和卷积层的梯度，并更新相应的参数 (Parameter)。
  - 最后，重复上述过程，直到损失函数收敛或达到预设的训练轮数。
- ViT-B/16 模型的前向传播过程如下：
  - 首先，将输入图像切分成 16x16 的小块，并将每个小块展平成一个 256 维的向量，然后再加上一个位置编码向量和一个类别嵌入向量，得到一个序列 (Sequence)。
  - 然后，将序列依次经过 12 层 Transformer 编码器，每层 Transformer 编码器包含一个多头自注意力子层和一个前馈神经网络子层，以及两个残差连接和两个层归一化操作，得到一个新的序列。
  - 最后，将新序列的第一个位置的向量经过一个线性层和一个 softmax 层，得到最终的输出向量，表示每个场景标签的概率。
- ViT-B/16 模型的反向传播过程如下：
  - 首先，计算输出向量和真实标签之间的交叉熵损失函数，并对损失函数求导，得到线性层和 softmax 层的梯度。
  - 然后，根据链式法则，依次计算 Transformer 编码器、位置编码、类别嵌入和图像切分的梯度，并更新相应的参数。
  - 最后，重复上述过程，直到损失函数收敛或达到预设的训练轮数。
```
